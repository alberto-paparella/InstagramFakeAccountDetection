{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram Fake Account Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.normalizer import json_importer_full\n",
    "from dataset.utils import shuffle_and_split\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading from file ./dataset/sources/automatedAccountData.json...\n",
      "Loaded 700 entries from source ./dataset/sources/automatedAccountData.json\n",
      "Now loading from file ./dataset/sources/nonautomatedAccountData.json...\n",
      "Loaded 700 entries from source ./dataset/sources/nonautomatedAccountData.json\n"
     ]
    }
   ],
   "source": [
    "fake = json_importer_full(\"./dataset/sources/automatedAccountData.json\", True)\n",
    "correct = json_importer_full(\"./dataset/sources/nonautomatedAccountData.json\", False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT 29 - 06"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP = 100 # Number of experiments\n",
    "MAX_ITER = 50000 # Maximum number of iterations for LR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using custom experiment functions not to mess up with the real experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Drop target columns from dataset\n",
    "'''\n",
    "def get_custom_dataset(train_df, validation_df, column_names=[]):\n",
    "    custom_train_df = train_df.drop(column_names, axis=1)\n",
    "    custom_validation_df = validation_df.drop(column_names, axis=1)\n",
    "\n",
    "    return custom_train_df, custom_validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "column_names: list of columns to drop from default dataset to get custom dataset\n",
    "\n",
    "modes:\n",
    " - \"dt\" => DecisionTree\n",
    " - \"lr\" => LogisticRegression\n",
    "'''\n",
    "def experiment(fake, correct, column_names=[], mode=\"dt\", n_iter=N_EXP):\n",
    "    avg_scores = {\n",
    "        'default': {'precision': 0, 'accuracy': 0, 'recall': 0, 'f1': 0},\n",
    "        'custom': {'precision': 0, 'accuracy': 0, 'recall': 0, 'f1': 0}\n",
    "    }\n",
    "\n",
    "    if mode == \"dt\":\n",
    "        print(f\"Calculating precision and accuracy metrics for Decision Trees over {n_iter} times\")\n",
    "    elif mode == \"lr\":\n",
    "        print(f\"Calculating precision and accuracy metrics for Logistic Regression (max_iter={MAX_ITER}) over {n_iter} times\")\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # Get new train_df and validation_df, same for default and custom\n",
    "        train_df, validation_df = shuffle_and_split(fake, correct)\n",
    "        custom_train_df, custom_validation_df = get_custom_dataset(train_df, validation_df, column_names)\n",
    "\n",
    "        # Default mode\n",
    "        if mode == \"dt\":\n",
    "            # Get new Decision Tree\n",
    "            clf = tree.DecisionTreeClassifier()\n",
    "            clf = clf.fit(train_df.iloc[:, :-2], train_df.iloc[:, -1])\n",
    "        elif mode == \"lr\":\n",
    "            # Get new Logistic Regressor\n",
    "            clf = LogisticRegression(random_state=0, max_iter=MAX_ITER)\n",
    "            clf = clf.fit(train_df.iloc[:, :-2], train_df.iloc[:, -1])\n",
    "\n",
    "        # Get ground truth and predictions to measure performance\n",
    "        X_val, y_val = validation_df.iloc[:, :-2], validation_df.iloc[:, -1]\n",
    "        y_pred = clf.predict(X_val)\n",
    "\n",
    "        # Default scores\n",
    "        scores = get_scores(y_val, y_pred)\n",
    "        avg_scores['default']['precision'] += scores['precision']\n",
    "        avg_scores['default']['accuracy'] += scores['accuracy']\n",
    "        avg_scores['default']['recall'] += scores['recall']\n",
    "        avg_scores['default']['f1'] += scores['f1']\n",
    "\n",
    "        # Custom mode\n",
    "        if mode == \"dt\":\n",
    "            # Get new Decision Tree\n",
    "            clf = tree.DecisionTreeClassifier()\n",
    "            clf = clf.fit(custom_train_df.iloc[:, :-2], custom_train_df.iloc[:, -1])\n",
    "        elif mode == \"lr\":\n",
    "            # Get new Logistic Regressor\n",
    "            clf = LogisticRegression(random_state=0, max_iter=2500)\n",
    "            clf = clf.fit(custom_train_df.iloc[:, :-2], custom_train_df.iloc[:, -1])\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "        # Get ground truth and predictions to measure performance\n",
    "        X_val, y_val = custom_validation_df.iloc[:, :-2], custom_validation_df.iloc[:, -1]\n",
    "        y_pred = clf.predict(X_val)\n",
    "\n",
    "        # Custom scores\n",
    "        scores = get_scores(y_val, y_pred)\n",
    "        avg_scores['custom']['precision'] += scores['precision']\n",
    "        avg_scores['custom']['accuracy'] += scores['accuracy']\n",
    "        avg_scores['custom']['recall'] += scores['recall']\n",
    "        avg_scores['custom']['f1'] += scores['f1']\n",
    "\n",
    "        #print(f\"{i + 1}/{n_iter}\", end=\"\\r\")\n",
    "\n",
    "    # Averaging\n",
    "    for t in avg_scores.keys():\n",
    "        for s in avg_scores[t].keys():\n",
    "            avg_scores[t][s] /= n_iter\n",
    "\n",
    "    print('Done!\\n\\n')\n",
    "\n",
    "    print('default avg precision:', \"{:.5f}\".format(avg_scores['default']['precision']))\n",
    "    print('default avg accuracy:', \"{:.5f}\".format(avg_scores['default']['accuracy']))\n",
    "    print('default avg recall:', \"{:.5f}\".format(avg_scores['default']['recall']))\n",
    "    print('default avg f1-score:', \"{:.5f}\".format(avg_scores['default']['f1']))\n",
    "    print('---')\n",
    "    print('custom avg precision:', \"{:.5f}\".format(avg_scores['custom']['precision']))\n",
    "    print('custom avg accuracy:', \"{:.5f}\".format(avg_scores['custom']['accuracy']))\n",
    "    print('custom avg recall:', \"{:.5f}\".format(avg_scores['custom']['recall']))\n",
    "    print('custom avg f1-score:', \"{:.5f}\".format(avg_scores['custom']['f1']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y_val, y_pred):\n",
    "    scores = {\n",
    "        'accuracy': 0,\n",
    "        'precision': 0,\n",
    "        'recall': 0,\n",
    "        'f1': 0\n",
    "    }\n",
    "    scores['accuracy'] += metrics.accuracy_score(y_val, y_pred)\n",
    "    scores['precision'] += metrics.precision_score(y_val, y_pred)\n",
    "    scores['recall'] += metrics.recall_score(y_val, y_pred)\n",
    "    scores['f1'] += metrics.f1_score(y_val, y_pred)\n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate impact upon removing single-attributes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact (bad/good) on performance is also evaluated from 1 (very small) to 5 (very big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['nmedia', 'biol', 'url', 'nfollowing', 'nfollower', 'mediaLikeNumbers',\n",
      "       'mediaHashtagNumbers', 'followerToFollowing', 'hasMedia',\n",
      "       'userHasHighlighReels', 'usernameLength', 'usernameDigitCount', 'fake'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame.from_dict(fake).columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nmedia (keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.90915\n",
      "default avg accuracy: 0.91128\n",
      "default avg recall: 0.91441\n",
      "default avg f1-score: 0.91156\n",
      "---\n",
      "custom avg precision: 0.90841\n",
      "custom avg accuracy: 0.91007\n",
      "custom avg recall: 0.91251\n",
      "custom avg f1-score: 0.91028\n"
     ]
    }
   ],
   "source": [
    "experiment(fake, correct, ['nmedia'], \"dt\")   # DecisionTree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### biol (keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.90672\n",
      "default avg accuracy: 0.90725\n",
      "default avg recall: 0.90844\n",
      "default avg f1-score: 0.90731\n",
      "---\n",
      "custom avg precision: 0.89562\n",
      "custom avg accuracy: 0.89808\n",
      "custom avg recall: 0.90161\n",
      "custom avg f1-score: 0.89839\n"
     ]
    }
   ],
   "source": [
    "experiment(fake, correct, ['biol'], \"dt\")   # DecisionTree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### url (drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.91078\n",
      "default avg accuracy: 0.90993\n",
      "default avg recall: 0.90953\n",
      "default avg f1-score: 0.90989\n",
      "---\n",
      "custom avg precision: 0.91180\n",
      "custom avg accuracy: 0.91078\n",
      "custom avg recall: 0.91009\n",
      "custom avg f1-score: 0.91071\n"
     ]
    }
   ],
   "source": [
    "experiment(fake, correct, ['url'], \"dt\")   # DecisionTree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nfollowing (try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.90314\n",
      "default avg accuracy: 0.90531\n",
      "default avg recall: 0.90844\n",
      "default avg f1-score: 0.90556\n",
      "---\n",
      "custom avg precision: 0.90579\n",
      "custom avg accuracy: 0.90588\n",
      "custom avg recall: 0.90649\n",
      "custom avg f1-score: 0.90592\n"
     ]
    }
   ],
   "source": [
    "experiment(fake, correct, ['nfollowing'], \"dt\")   # DecisionTree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nfollower (try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.90400\n",
      "default avg accuracy: 0.90583\n",
      "default avg recall: 0.90863\n",
      "default avg f1-score: 0.90605\n",
      "---\n",
      "custom avg precision: 0.91011\n",
      "custom avg accuracy: 0.90623\n",
      "custom avg recall: 0.90190\n",
      "custom avg f1-score: 0.90575\n"
     ]
    }
   ],
   "source": [
    "experiment(fake, correct, ['nfollower'], \"dt\")   # DecisionTree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mediaLikeNumbers (drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.90746\n",
      "default avg accuracy: 0.90730\n",
      "default avg recall: 0.90754\n",
      "default avg f1-score: 0.90730\n",
      "---\n",
      "custom avg precision: 0.90885\n",
      "custom avg accuracy: 0.90948\n",
      "custom avg recall: 0.91071\n",
      "custom avg f1-score: 0.90957\n"
     ]
    }
   ],
   "source": [
    "experiment(fake, correct, ['mediaLikeNumbers'], \"dt\")   # DecisionTree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mediaHashtagNumbers (keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.90920\n",
      "default avg accuracy: 0.90642\n",
      "default avg recall: 0.90355\n",
      "default avg f1-score: 0.90615\n",
      "---\n",
      "custom avg precision: 0.90646\n",
      "custom avg accuracy: 0.90282\n",
      "custom avg recall: 0.89886\n",
      "custom avg f1-score: 0.90239\n"
     ]
    }
   ],
   "source": [
    "experiment(fake, correct, ['mediaHashtagNumbers'], \"dt\")   # DecisionTree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### followerToFollowing (keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.91026\n",
      "default avg accuracy: 0.90799\n",
      "default avg recall: 0.90578\n",
      "default avg f1-score: 0.90776\n",
      "---\n",
      "custom avg precision: 0.90432\n",
      "custom avg accuracy: 0.89912\n",
      "custom avg recall: 0.89336\n",
      "custom avg f1-score: 0.89852\n"
     ]
    }
   ],
   "source": [
    "experiment(fake, correct, ['followerToFollowing'], \"dt\")   # DecisionTree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hasMedia (try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.90882\n",
      "default avg accuracy: 0.90938\n",
      "default avg recall: 0.91076\n",
      "default avg f1-score: 0.90950\n",
      "---\n",
      "custom avg precision: 0.90924\n",
      "custom avg accuracy: 0.90919\n",
      "custom avg recall: 0.90986\n",
      "custom avg f1-score: 0.90923\n"
     ]
    }
   ],
   "source": [
    "experiment(fake, correct, ['hasMedia'], \"dt\")   # DecisionTree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### userHasHighlighReels (try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.90971\n",
      "default avg accuracy: 0.90820\n",
      "default avg recall: 0.90678\n",
      "default avg f1-score: 0.90803\n",
      "---\n",
      "custom avg precision: 0.91151\n",
      "custom avg accuracy: 0.90903\n",
      "custom avg recall: 0.90640\n",
      "custom avg f1-score: 0.90874\n"
     ]
    }
   ],
   "source": [
    "experiment(fake, correct, ['userHasHighlighReels'], \"dt\")   # DecisionTree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usernameLength (drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.90698\n",
      "default avg accuracy: 0.90735\n",
      "default avg recall: 0.90839\n",
      "default avg f1-score: 0.90741\n",
      "---\n",
      "custom avg precision: 0.90907\n",
      "custom avg accuracy: 0.90969\n",
      "custom avg recall: 0.91114\n",
      "custom avg f1-score: 0.90981\n"
     ]
    }
   ],
   "source": [
    "experiment(fake, correct, ['usernameLength'], \"dt\")   # DecisionTree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usernameDigitCount (drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.90527\n",
      "default avg accuracy: 0.90614\n",
      "default avg recall: 0.90777\n",
      "default avg f1-score: 0.90626\n",
      "---\n",
      "custom avg precision: 0.90868\n",
      "custom avg accuracy: 0.90922\n",
      "custom avg recall: 0.91052\n",
      "custom avg f1-score: 0.90933\n"
     ]
    }
   ],
   "source": [
    "experiment(fake, correct, ['usernameDigitCount'], \"dt\")   # DecisionTree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram Fake Account Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.normalizer import csv_importer_full\n",
    "from dataset.utils import find_demarcator, shuffle_and_split\n",
    "from sequoia_comparison.utils import get_scores\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading from file dataset/sources/user_fake_authentic_2class.csv...\n",
      "Loaded 65327 entries from source dataset/sources/user_fake_authentic_2class.csv\n"
     ]
    }
   ],
   "source": [
    "default_dataset = csv_importer_full(\"dataset/sources/user_fake_authentic_2class.csv\")\n",
    "idx = find_demarcator(default_dataset)\n",
    "\n",
    "fake = default_dataset[:idx]\n",
    "correct = default_dataset[idx:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT 26 - 04"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXP = 100 # Number of experiments\n",
    "MAX_ITER = 50000 # Maximum number of iterations for LR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using custom experiment functions not to mess up with the real experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Drop target columns from dataset\n",
    "'''\n",
    "def get_custom_dataset(train_df, validation_df, column_names=[]):\n",
    "    custom_train_df = train_df.drop(column_names, axis=1)\n",
    "    custom_validation_df = validation_df.drop(column_names, axis=1)\n",
    "\n",
    "    return custom_train_df, custom_validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "column_names: list of columns to drop from default dataset to get custom dataset\n",
    "\n",
    "modes:\n",
    " - \"dt\" => DecisionTree\n",
    " - \"lr\" => LogisticRegression\n",
    "'''\n",
    "def experiment(fake, correct, column_names=[], mode=\"dt\", n_iter=N_EXP):\n",
    "    avg_scores = {\n",
    "        'default': {'precision': 0, 'accuracy': 0},\n",
    "        'custom': {'precision': 0, 'accuracy': 0}\n",
    "    }\n",
    "\n",
    "    if mode == \"dt\":\n",
    "        print(f\"Calculating precision and accuracy metrics for Decision Trees over {n_iter} times\")\n",
    "    elif mode == \"lr\":\n",
    "        print(f\"Calculating precision and accuracy metrics for Logistic Regression (max_iter={MAX_ITER}) over {n_iter} times\")\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # Get new train_df and validation_df, same for default and custom\n",
    "        train_df, validation_df = shuffle_and_split(fake, correct)\n",
    "        custom_train_df, custom_validation_df = get_custom_dataset(train_df, validation_df, column_names)\n",
    "\n",
    "        # Default mode\n",
    "        if mode == \"dt\":\n",
    "            # Get new Decision Tree\n",
    "            clf = tree.DecisionTreeClassifier()\n",
    "            clf = clf.fit(train_df.iloc[:, :-2], train_df.iloc[:, -1])\n",
    "        elif mode == \"lr\":\n",
    "            # Get new Logistic Regressor\n",
    "            clf = LogisticRegression(random_state=0, max_iter=MAX_ITER)\n",
    "            clf = clf.fit(train_df.iloc[:, :-2], train_df.iloc[:, -1])\n",
    "\n",
    "        # Get ground truth and predictions to measure performance\n",
    "        X_val, y_val = validation_df.iloc[:, :-2], validation_df.iloc[:, -1]\n",
    "        y_pred = clf.predict(X_val)\n",
    "\n",
    "        # Default scores\n",
    "        scores = get_scores(y_val, y_pred)\n",
    "        avg_scores['default']['precision'] += scores['precision']\n",
    "        avg_scores['default']['accuracy'] += scores['accuracy']\n",
    "\n",
    "        # Custom mode\n",
    "        if mode == \"dt\":\n",
    "            # Get new Decision Tree\n",
    "            clf = tree.DecisionTreeClassifier()\n",
    "            clf = clf.fit(custom_train_df.iloc[:, :-2], custom_train_df.iloc[:, -1])\n",
    "        elif mode == \"lr\":\n",
    "            # Get new Logistic Regressor\n",
    "            clf = LogisticRegression(random_state=0, max_iter=2500)\n",
    "            clf = clf.fit(custom_train_df.iloc[:, :-2], custom_train_df.iloc[:, -1])\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "        # Get ground truth and predictions to measure performance\n",
    "        X_val, y_val = custom_validation_df.iloc[:, :-2], custom_validation_df.iloc[:, -1]\n",
    "        y_pred = clf.predict(X_val)\n",
    "\n",
    "        # Custom scores\n",
    "        scores = get_scores(y_val, y_pred)\n",
    "        avg_scores['custom']['precision'] += scores['precision']\n",
    "        avg_scores['custom']['accuracy'] += scores['accuracy']\n",
    "\n",
    "        #print(f\"{i + 1}/{n_iter}\", end=\"\\r\")\n",
    "\n",
    "    # Averaging\n",
    "    for t in avg_scores.keys():\n",
    "        for s in avg_scores[t].keys():\n",
    "            avg_scores[t][s] /= n_iter\n",
    "\n",
    "    print('Done!\\n\\n')\n",
    "\n",
    "    print('default avg precision:', \"{:.5f}\".format(avg_scores['default']['precision']))\n",
    "    print('default avg accuracy:', \"{:.5f}\".format(avg_scores['default']['accuracy']))\n",
    "\n",
    "    print('custom avg precision:', \"{:.5f}\".format(avg_scores['custom']['precision']))\n",
    "    print('custom avg accuracy:', \"{:.5f}\".format(avg_scores['custom']['accuracy']))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate impact upon removing single-attributes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact (bad/good) on performance is also evaluated from 1 (very small) to 5 (very big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['nmedia', 'flw', 'flg', 'biol', 'pic', 'url', 'cl', 'cz', 'ni', 'erl',\n",
      "       'erc', 'lt', 'ahc', 'pr', 'fo', 'cs', 'avgtime', 'fake'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame.from_dict(fake).columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nmedia - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85094\n",
      "default avg accuracy: 0.85395\n",
      "custom avg precision: 0.84954\n",
      "custom avg accuracy: 0.85216\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.81017\n",
      "default avg accuracy: 0.79699\n",
      "custom avg precision: 0.80927\n",
      "custom avg accuracy: 0.79603\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['nmedia'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['nmedia'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85094\n",
    "# default avg accuracy: 0.85395\n",
    "# custom avg precision: 0.84954\n",
    "# custom avg accuracy: 0.85216\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.81017\n",
    "# default avg accuracy: 0.79699\n",
    "# custom avg precision: 0.80927\n",
    "# custom avg accuracy: 0.79603"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing nmedia has a bad impact on performance - KEEP IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flw - K (C - LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85141\n",
      "default avg accuracy: 0.85402\n",
      "custom avg precision: 0.83480\n",
      "custom avg accuracy: 0.83765\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80972\n",
      "default avg accuracy: 0.79626\n",
      "custom avg precision: 0.80990\n",
      "custom avg accuracy: 0.79693\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['flw'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['flw'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85141\n",
    "# default avg accuracy: 0.85402\n",
    "# custom avg precision: 0.83480\n",
    "# custom avg accuracy: 0.83765\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80972\n",
    "# default avg accuracy: 0.79626\n",
    "# custom avg precision: 0.80990\n",
    "# custom avg accuracy: 0.79693"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing flw has a bad impact on performace on DT, but positive on LR - CONSIDER DROPPING IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flg - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85214\n",
      "default avg accuracy: 0.85457\n",
      "custom avg precision: 0.80051\n",
      "custom avg accuracy: 0.80298\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80903\n",
      "default avg accuracy: 0.79640\n",
      "custom avg precision: 0.74349\n",
      "custom avg accuracy: 0.74912\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['flg'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['flg'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85214\n",
    "# default avg accuracy: 0.85457\n",
    "# custom avg precision: 0.80051\n",
    "# custom avg accuracy: 0.80298\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80903\n",
    "# default avg accuracy: 0.79640\n",
    "# custom avg precision: 0.74349\n",
    "# custom avg accuracy: 0.74912"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing flg has a bad impact on performance - KEEP IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### biol - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85076\n",
      "default avg accuracy: 0.85367\n",
      "custom avg precision: 0.84963\n",
      "custom avg accuracy: 0.85216\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80935\n",
      "default avg accuracy: 0.79646\n",
      "custom avg precision: 0.80896\n",
      "custom avg accuracy: 0.79454\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['biol'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['biol'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85076\n",
    "# default avg accuracy: 0.85367\n",
    "# custom avg precision: 0.84963\n",
    "# custom avg accuracy: 0.85216\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80935\n",
    "# default avg accuracy: 0.79646\n",
    "# custom avg precision: 0.80896\n",
    "# custom avg accuracy: 0.79454"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing biol has a bad impact on performance - KEEP IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pic - D (C - DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85159\n",
      "default avg accuracy: 0.85417\n",
      "custom avg precision: 0.85190\n",
      "custom avg accuracy: 0.85439\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80914\n",
      "default avg accuracy: 0.79591\n",
      "custom avg precision: 0.80716\n",
      "custom avg accuracy: 0.79518\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['pic'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['pic'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85159\n",
    "# default avg accuracy: 0.85417\n",
    "# custom avg precision: 0.85190\n",
    "# custom avg accuracy: 0.85439\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80914\n",
    "# default avg accuracy: 0.79591\n",
    "# custom avg precision: 0.80716\n",
    "# custom avg accuracy: 0.79518"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing pic has a positive impact on performances on DT, but bad on LR - CONSIDER DROPPING IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### url - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85121\n",
      "default avg accuracy: 0.85404\n",
      "custom avg precision: 0.80325\n",
      "custom avg accuracy: 0.80367\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80994\n",
      "default avg accuracy: 0.79702\n",
      "custom avg precision: 0.78941\n",
      "custom avg accuracy: 0.76510\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['url'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['url'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85121\n",
    "# default avg accuracy: 0.85404\n",
    "# custom avg precision: 0.80325\n",
    "# custom avg accuracy: 0.80367\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80994\n",
    "# default avg accuracy: 0.79702\n",
    "# custom avg precision: 0.78941\n",
    "# custom avg accuracy: 0.76510"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing url has a bad impact on performace - KEEP IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cl - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85147\n",
      "default avg accuracy: 0.85408\n",
      "custom avg precision: 0.85019\n",
      "custom avg accuracy: 0.85321\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80894\n",
      "default avg accuracy: 0.79608\n",
      "custom avg precision: 0.80742\n",
      "custom avg accuracy: 0.79552\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['cl'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['cl'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85147\n",
    "# default avg accuracy: 0.85408\n",
    "# custom avg precision: 0.85019\n",
    "# custom avg accuracy: 0.85321\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80894\n",
    "# default avg accuracy: 0.79608\n",
    "# custom avg precision: 0.80742\n",
    "# custom avg accuracy: 0.79552"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing cl has a BAD impact on performance - KEEP IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cz - D (C - LR/BOTH?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85135\n",
      "default avg accuracy: 0.85380\n",
      "custom avg precision: 0.85122\n",
      "custom avg accuracy: 0.85381\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80933\n",
      "default avg accuracy: 0.79641\n",
      "custom avg precision: 0.81296\n",
      "custom avg accuracy: 0.79740\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['cz'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['cz'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85135\n",
    "# default avg accuracy: 0.85380\n",
    "# custom avg precision: 0.85122\n",
    "# custom avg accuracy: 0.85381\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80933\n",
    "# default avg accuracy: 0.79641\n",
    "# custom avg precision: 0.81296\n",
    "# custom avg accuracy: 0.79740"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing cz has a bad (?) impact on performace on DT, but positive on LR - CONSIDER DROPPING IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ni - K (C - LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85114\n",
      "default avg accuracy: 0.85380\n",
      "custom avg precision: 0.85048\n",
      "custom avg accuracy: 0.85331\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80995\n",
      "default avg accuracy: 0.79680\n",
      "custom avg precision: 0.81205\n",
      "custom avg accuracy: 0.79769\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['ni'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['ni'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85114\n",
    "# default avg accuracy: 0.85380\n",
    "# custom avg precision: 0.85048\n",
    "# custom avg accuracy: 0.85331\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80995\n",
    "# default avg accuracy: 0.79680\n",
    "# custom avg precision: 0.81205\n",
    "# custom avg accuracy: 0.79769"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing ni has a bad impact on performance on DT, but positive on LR - CONSIDER DROPPING IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### erl - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85144\n",
      "default avg accuracy: 0.85371\n",
      "custom avg precision: 0.83524\n",
      "custom avg accuracy: 0.83825\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80988\n",
      "default avg accuracy: 0.79642\n",
      "custom avg precision: 0.80894\n",
      "custom avg accuracy: 0.79603\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['erl'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['erl'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85144\n",
    "# default avg accuracy: 0.85371\n",
    "# custom avg precision: 0.83524\n",
    "# custom avg accuracy: 0.83825\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80988\n",
    "# default avg accuracy: 0.79642\n",
    "# custom avg precision: 0.80894\n",
    "# custom avg accuracy: 0.79603"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing erl has a bad impact on performace - KEEP IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### erc - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85207\n",
      "default avg accuracy: 0.85432\n",
      "custom avg precision: 0.82484\n",
      "custom avg accuracy: 0.82632\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80992\n",
      "default avg accuracy: 0.79728\n",
      "custom avg precision: 0.80638\n",
      "custom avg accuracy: 0.79195\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['erc'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['erc'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85207\n",
    "# default avg accuracy: 0.85432\n",
    "# custom avg precision: 0.82484\n",
    "# custom avg accuracy: 0.82632\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80992\n",
    "# default avg accuracy: 0.79728\n",
    "# custom avg precision: 0.80638\n",
    "# custom avg accuracy: 0.79195"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing erc has a bad impact on performace - KEEP IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lt - D (C - DT/BOTH?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85129\n",
      "default avg accuracy: 0.85399\n",
      "custom avg precision: 0.85167\n",
      "custom avg accuracy: 0.85433\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80996\n",
      "default avg accuracy: 0.79657\n",
      "custom avg precision: 0.81144\n",
      "custom avg accuracy: 0.78659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['lt'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['lt'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85129\n",
    "# default avg accuracy: 0.85399\n",
    "# custom avg precision: 0.85167\n",
    "# custom avg accuracy: 0.85433\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80996\n",
    "# default avg accuracy: 0.79657\n",
    "# custom avg precision: 0.81144\n",
    "# custom avg accuracy: 0.78659"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing lt has a positive impact on performace on DT, but bad (?) on LR - CONSIDER DROPING IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ahc - D (C - DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85075\n",
      "default avg accuracy: 0.85389\n",
      "custom avg precision: 0.85124\n",
      "custom avg accuracy: 0.85410\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80897\n",
      "default avg accuracy: 0.79657\n",
      "custom avg precision: 0.80818\n",
      "custom avg accuracy: 0.79578\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['ahc'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['ahc'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85075\n",
    "# default avg accuracy: 0.85389\n",
    "# custom avg precision: 0.85124\n",
    "# custom avg accuracy: 0.85410\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80897\n",
    "# default avg accuracy: 0.79657\n",
    "# custom avg precision: 0.80818\n",
    "# custom avg accuracy: 0.79578"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing ahc has a positive impact on performace on DT, but bad on LR - CONSIDER DROPPING IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pr - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85152\n",
      "default avg accuracy: 0.85438\n",
      "custom avg precision: 0.84928\n",
      "custom avg accuracy: 0.85233\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.81019\n",
      "default avg accuracy: 0.79716\n",
      "custom avg precision: 0.80676\n",
      "custom avg accuracy: 0.79327\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['pr'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['pr'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85152\n",
    "# default avg accuracy: 0.85438\n",
    "# custom avg precision: 0.84928\n",
    "# custom avg accuracy: 0.85233\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.81019\n",
    "# default avg accuracy: 0.79716\n",
    "# custom avg precision: 0.80676\n",
    "# custom avg accuracy: 0.79327"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing pr has a bad impact on performace - KEEP IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fo - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85148\n",
      "default avg accuracy: 0.85402\n",
      "custom avg precision: 0.85037\n",
      "custom avg accuracy: 0.85301\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80995\n",
      "default avg accuracy: 0.79711\n",
      "custom avg precision: 0.80904\n",
      "custom avg accuracy: 0.79569\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['fo'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['fo'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85148\n",
    "# default avg accuracy: 0.85402\n",
    "# custom avg precision: 0.85037\n",
    "# custom avg accuracy: 0.85301\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80995\n",
    "# default avg accuracy: 0.79711\n",
    "# custom avg precision: 0.80904\n",
    "# custom avg accuracy: 0.79569"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing fo has a bad impact on performace - KEEP IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cs - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85115\n",
      "default avg accuracy: 0.85405\n",
      "custom avg precision: 0.85046\n",
      "custom avg accuracy: 0.85358\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80959\n",
      "default avg accuracy: 0.79664\n",
      "custom avg precision: 0.78385\n",
      "custom avg accuracy: 0.78669\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['cs'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['cs'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85115\n",
    "# default avg accuracy: 0.85405\n",
    "# custom avg precision: 0.85046\n",
    "# custom avg accuracy: 0.85358\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80959\n",
    "# default avg accuracy: 0.79664\n",
    "# custom avg precision: 0.78385\n",
    "# custom avg accuracy: 0.78669"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing cs has a small bad impact on performace - KEEP IT !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avgtime - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85115\n",
      "default avg accuracy: 0.85387\n",
      "custom avg precision: 0.85046\n",
      "custom avg accuracy: 0.85337\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80965\n",
      "default avg accuracy: 0.79644\n",
      "custom avg precision: 0.78246\n",
      "custom avg accuracy: 0.78587\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, ['avgtime'], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, ['avgtime'], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85115\n",
    "# default avg accuracy: 0.85387\n",
    "# custom avg precision: 0.85046\n",
    "# custom avg accuracy: 0.85337\n",
    "# Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80965\n",
    "# default avg accuracy: 0.79644\n",
    "# custom avg precision: 0.78246\n",
    "# custom avg accuracy: 0.78587"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION: removing flw has a positive impact on performace - KEEP IT !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second experiment with custom features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we fit (several) Decision Tree Classifier(s) (and Linear Regressors) removing from dataframes the attributes which seemed to worsen performance during the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85160\n",
      "default avg accuracy: 0.85414\n",
      "custom avg precision: 0.83241\n",
      "custom avg accuracy: 0.83532\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.81018\n",
      "default avg accuracy: 0.79672\n",
      "custom avg precision: 0.81582\n",
      "custom avg accuracy: 0.78348\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, [\"flw\", \"cz\", \"pic\", \"ni\", \"lt\", \"ahc\"], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, [\"flw\", \"pic\", \"cz\", \"ni\", \"lt\", \"ahc\"], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 50 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85170\n",
    "# default avg accuracy: 0.85395\n",
    "# custom avg precision: 0.83050\n",
    "# custom avg accuracy: 0.83394\n",
    "# Calculating precision and accuracy metrics for Logistic Regression over 50 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80960\n",
    "# default avg accuracy: 0.79711\n",
    "# custom avg precision: 0.78490\n",
    "# custom avg accuracy: 0.75939"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONLUSION: IT FAILED !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we fit (several) Decision Tree Classifier(s) (and Linear Regressors) removing from dataframes the attributes which seemed to worsen performance (of Decision Trees only ! ) during the experiments.\n",
    "\n",
    "(HT: LR depends a lot more on MAX_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating precision and accuracy metrics for Decision Trees over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.85181\n",
      "default avg accuracy: 0.85407\n",
      "custom avg precision: 0.85078\n",
      "custom avg accuracy: 0.85347\n",
      "Calculating precision and accuracy metrics for Logistic Regression (max_iter=25000) over 100 times\n",
      "Done!\n",
      "\n",
      "\n",
      "default avg precision: 0.80964\n",
      "default avg accuracy: 0.79689\n",
      "custom avg precision: 0.80868\n",
      "custom avg accuracy: 0.78385\n"
     ]
    }
   ],
   "source": [
    "# Experiments\n",
    "experiment(fake, correct, [\"pic\", \"cz\", \"lt\", \"ahc\"], \"dt\")   # DecisionTree\n",
    "experiment(fake, correct, [\"pic\", \"cz\", \"lt\", \"ahc\"], \"lr\")   # LogisticRegression\n",
    "\n",
    "# LOG\n",
    "# Calculating precision and accuracy metrics for Decision Trees over 50 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.85070\n",
    "# default avg accuracy: 0.85364\n",
    "# custom avg precision: 0.85122\n",
    "# custom avg accuracy: 0.85394\n",
    "# Calculating precision and accuracy metrics for Logistic Regression over 50 times\n",
    "# Done!\n",
    "\n",
    "\n",
    "# default avg precision: 0.80923\n",
    "# default avg accuracy: 0.79600\n",
    "# custom avg precision: 0.77695\n",
    "# custom avg accuracy: 0.76550"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSIONS: removing ni, lt, ahc and avgtime improved DT but worsened LR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

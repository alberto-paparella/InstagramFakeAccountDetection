{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram Fake Account Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.normalizer import csv_importer, csv_importer_full\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn import tree, metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to find demarcator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_demarcator(dataset):\n",
    "    \"\"\"\n",
    "    Restituisce l'indice del primo elemento non fake\n",
    "    :param dataset: il dataset\n",
    "    :return: l'indice\n",
    "    \"\"\"\n",
    "    idx = 0\n",
    "    for elem in dataset:\n",
    "        if elem['fake'] == 1:\n",
    "            idx += 1\n",
    "        else:\n",
    "            break\n",
    "    return idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set train:test ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENT_TRAIN = 70"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading from file ./dataset/sources/user_fake_authentic_2class.csv...\n",
      "Loaded 65327 entries from source ./dataset/sources/user_fake_authentic_2class.csv\n"
     ]
    }
   ],
   "source": [
    "default_dataset = csv_importer_full(\"./dataset/sources/user_fake_authentic_2class.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into (balanced) training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now splitting dataset with ratio 70:30\n",
      "Loading complete.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Now splitting dataset with ratio {PERCENT_TRAIN}:{100 - PERCENT_TRAIN}\")\n",
    "\n",
    "# Find demarcator (in the original datasets all fake accounts are at the beginning)\n",
    "idx = find_demarcator(default_dataset)\n",
    "# Separate fakes from real accounts\n",
    "fake = default_dataset[:idx]\n",
    "correct = default_dataset[idx:]\n",
    "# Shuffle both datatets (otherwise, train and validation sets would always contain the same elements)\n",
    "random.shuffle(fake)\n",
    "random.shuffle(correct)\n",
    "# Create training set\n",
    "train = fake[:int(len(fake) * (PERCENT_TRAIN / 100))]\n",
    "train += correct[:int(len(correct) * (PERCENT_TRAIN / 100))]\n",
    "# Create validation set\n",
    "validation = fake[int(len(fake) * (PERCENT_TRAIN / 100)):]\n",
    "validation += correct[int(len(correct) * (PERCENT_TRAIN / 100)):]\n",
    "# Shuffle both datasets\n",
    "random.shuffle(train)\n",
    "random.shuffle(validation)\n",
    "\n",
    "print(\"Loading complete.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast to pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       nmedia     flw     flg   biol  pic  url     cl        cz     ni  \\\n",
      "0         7.0    92.0  6700.0  136.0  1.0  0.0   31.0  0.000000  0.000   \n",
      "1        13.0   365.0  4100.0    0.0  1.0  0.0    4.0  0.538462  0.231   \n",
      "2         2.0    11.0   125.0    0.0  0.0  0.0    0.0  1.000000  0.500   \n",
      "3        72.0   311.0  7400.0    0.0  1.0  0.0    1.0  0.944444  0.000   \n",
      "4         0.0  2300.0  7500.0   12.0  0.0  0.0    0.0  0.000000  0.000   \n",
      "...       ...     ...     ...    ...  ...  ...    ...       ...    ...   \n",
      "45723    66.0   277.0   469.0  149.0  1.0  1.0   64.0  0.944444  0.000   \n",
      "45724     1.0   140.0  7300.0    0.0  1.0  0.0    0.0  1.000000  0.000   \n",
      "45725     1.0   431.0  7300.0   59.0  1.0  0.0    0.0  1.000000  1.000   \n",
      "45726    71.0   279.0  2200.0    0.0  1.0  0.0    1.0  0.944444  0.000   \n",
      "45727    81.0   577.0  1300.0   62.0  1.0  0.0  449.0  0.000000  0.333   \n",
      "\n",
      "              erl   erc     lt    ahc     pr     fo        cs     avgtime  \\\n",
      "0       55.119999  2.02  0.000  2.286  0.000  0.000  0.020944    7.484246   \n",
      "1       26.830000  0.19  0.231  0.000  0.000  0.000  0.346154  419.510315   \n",
      "2       50.000000  9.09  0.000  0.000  0.000  0.000  1.000000    0.010139   \n",
      "3        3.610000  0.05  0.000  0.000  0.000  0.000  0.888889   71.889610   \n",
      "4        0.000000  0.00  0.000  0.000  0.000  0.000  1.000000    0.000000   \n",
      "...           ...   ...    ...    ...    ...    ...       ...         ...   \n",
      "45723    2.990000  0.02  0.000  1.056  0.000  0.056  0.888889   54.541313   \n",
      "45724  187.139999  6.43  0.000  0.000  0.000  0.000  0.000000    0.000000   \n",
      "45725   12.300000  0.23  0.000  0.000  0.000  0.000  0.000000    0.000000   \n",
      "45726    9.720000  1.08  0.111  0.000  0.000  0.000  0.888889    0.016235   \n",
      "45727    3.530000  0.51  0.000  0.333  0.167  0.000  0.063547  680.825134   \n",
      "\n",
      "       fake  \n",
      "0         1  \n",
      "1         1  \n",
      "2         1  \n",
      "3         1  \n",
      "4         1  \n",
      "...     ...  \n",
      "45723     0  \n",
      "45724     1  \n",
      "45725     1  \n",
      "45726     0  \n",
      "45727     1  \n",
      "\n",
      "[45728 rows x 18 columns]\n",
      "       nmedia    flw     flg   biol  pic  url     cl        cz     ni  \\\n",
      "0        12.0   62.0   503.0    0.0  1.0  0.0   29.0  0.250000  0.417   \n",
      "1       241.0  164.0  1300.0    9.0  1.0  0.0  332.0  0.055556  0.611   \n",
      "2         2.0   44.0  6300.0    0.0  0.0  0.0    7.0  0.000000  0.000   \n",
      "3       288.0  225.0  1400.0    0.0  1.0  0.0  440.0  0.000000  0.000   \n",
      "4        64.0  749.0   936.0   28.0  1.0  1.0  470.0  0.000000  0.056   \n",
      "...       ...    ...     ...    ...  ...  ...    ...       ...    ...   \n",
      "19593   364.0  177.0   192.0  156.0  1.0  1.0  953.0  0.000000  0.167   \n",
      "19594    53.0  502.0     2.0   66.0  1.0  1.0  188.0  0.055556  0.278   \n",
      "19595     0.0  106.0   773.0   11.0  1.0  0.0    0.0  0.000000  0.000   \n",
      "19596     7.0   45.0   794.0    0.0  1.0  0.0   15.0  0.428571  0.000   \n",
      "19597    83.0  290.0  2000.0   12.0  1.0  0.0    6.0  0.611111  0.167   \n",
      "\n",
      "              erl    erc     lt    ahc     pr     fo        cs     avgtime  \\\n",
      "0       16.260000   0.67  0.000  0.000  0.000  0.000  0.061692   61.633495   \n",
      "1        7.280000   0.68  0.389  0.333  0.000  0.000  0.069291  544.568909   \n",
      "2      322.730011  14.77  0.000  0.000  0.000  0.000  0.000000    0.074722   \n",
      "3        3.700000   0.27  0.000  1.722  0.000  0.000  0.897357   21.613626   \n",
      "4       12.540000   1.91  0.000  1.611  0.000  0.000  0.816281  717.277832   \n",
      "...           ...    ...    ...    ...    ...    ...       ...         ...   \n",
      "19593    2.100000   0.16  0.389  0.444  0.056  0.000  0.116091   52.183842   \n",
      "19594    8.840000   0.09  0.000  0.333  0.000  0.056  0.123032  282.609406   \n",
      "19595    0.000000   0.00  0.000  0.000  0.000  0.000  1.000000    0.000000   \n",
      "19596   41.270000   0.63  0.143  0.571  0.000  0.000  0.142857    5.232302   \n",
      "19597   10.440000   0.23  0.333  0.278  0.000  0.000  0.366013   18.848364   \n",
      "\n",
      "       fake  \n",
      "0         1  \n",
      "1         1  \n",
      "2         1  \n",
      "3         1  \n",
      "4         0  \n",
      "...     ...  \n",
      "19593     0  \n",
      "19594     0  \n",
      "19595     1  \n",
      "19596     1  \n",
      "19597     1  \n",
      "\n",
      "[19598 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame.from_dict(train)\n",
    "validation_df = pd.DataFrame.from_dict(validation)\n",
    "print(train_df)\n",
    "print(validation_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting complete.\n"
     ]
    }
   ],
   "source": [
    "# Default tree\n",
    "X, y = train_df.iloc[:, :-2], train_df.iloc[:, -1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "print(\"Fitting complete.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85      9738\n",
      "           1       0.85      0.86      0.86      9860\n",
      "\n",
      "    accuracy                           0.85     19598\n",
      "   macro avg       0.85      0.85      0.85     19598\n",
      "weighted avg       0.85      0.85      0.85     19598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = validation_df.iloc[:, :-2], validation_df.iloc[:, -1]\n",
    "y_pred = clf.predict(X_val)\n",
    "print(metrics.classification_report(y_val,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First experiment with custom features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading from file ./dataset/sources/user_fake_authentic_2class.csv...\n",
      "Loaded 65327 entries from source ./dataset/sources/user_fake_authentic_2class.csv\n",
      "Loading complete.\n",
      "Fitting complete.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.49      0.49      9738\n",
      "           1       0.50      0.51      0.51      9860\n",
      "\n",
      "    accuracy                           0.50     19598\n",
      "   macro avg       0.50      0.50      0.50     19598\n",
      "weighted avg       0.50      0.50      0.50     19598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_dataset = csv_importer(\"./dataset/sources/user_fake_authentic_2class.csv\")\n",
    "\n",
    "custom_fake = custom_dataset[:idx]\n",
    "custom_correct = custom_dataset[idx:]\n",
    "\n",
    "random.shuffle(custom_fake)\n",
    "random.shuffle(custom_correct)\n",
    "\n",
    "custom_train = custom_fake[:int(len(custom_fake) * (PERCENT_TRAIN / 100))]\n",
    "custom_train += custom_correct[:int(len(custom_correct) * (PERCENT_TRAIN / 100))]\n",
    "\n",
    "custom_validation = custom_fake[int(len(custom_fake) * (PERCENT_TRAIN / 100)):]\n",
    "custom_validation += custom_correct[int(len(custom_correct) * (PERCENT_TRAIN / 100)):]\n",
    "\n",
    "random.shuffle(custom_train)\n",
    "random.shuffle(custom_validation)\n",
    "\n",
    "print(\"Loading complete.\")\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(train)\n",
    "validation_df = pd.DataFrame.from_dict(validation)\n",
    "#print(train_df)\n",
    "#print(validation_df)\n",
    "\n",
    "custom_train_df = pd.DataFrame.from_dict(custom_train)\n",
    "custom_validation_df = pd.DataFrame.from_dict(custom_validation)\n",
    "#print(custom_train_df)\n",
    "#print(custom_validation_df)\n",
    "\n",
    "# Custom tree\n",
    "cX, cy = custom_train_df.iloc[:,:-2], custom_train_df.iloc[:,-1]\n",
    "cclf = tree.DecisionTreeClassifier()\n",
    "cclf = cclf.fit(cX, cy)\n",
    "print(\"Fitting complete.\")\n",
    "\n",
    "cX_val, cy_val = custom_validation_df.iloc[:,:-2], validation_df.iloc[:, -1]\n",
    "cy_pred = cclf.predict(cX_val)\n",
    "\n",
    "print(metrics.classification_report(cy_val,cy_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate impact upon removing single-attributes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nmedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['nmedia', 'flw', 'flg', 'biol', 'pic', 'url', 'cl', 'cz', 'ni', 'erl',\n",
      "       'erc', 'lt', 'ahc', 'pr', 'fo', 'cs', 'avgtime', 'fake'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)\n",
    "#print(train_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove nmedia column from training and validation dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drop_nmedia = train_df.drop(['nmedia'], axis=1)\n",
    "validation_drop_nmedia = validation_df.drop(['nmedia'], axis=1)\n",
    "#print(train_drop_nmedia)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting complete.\n"
     ]
    }
   ],
   "source": [
    "X, y = train_drop_nmedia.iloc[:, :-2], train_drop_nmedia.iloc[:, -1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "print(\"Fitting complete.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85      9738\n",
      "           1       0.85      0.86      0.85      9860\n",
      "\n",
      "    accuracy                           0.85     19598\n",
      "   macro avg       0.85      0.85      0.85     19598\n",
      "weighted avg       0.85      0.85      0.85     19598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = validation_drop_nmedia.iloc[:, :-2], validation_drop_nmedia.iloc[:, -1]\n",
    "y_pred = clf.predict(X_val)\n",
    "print(metrics.classification_report(y_val,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

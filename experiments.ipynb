{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram Fake Account Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.normalizer import csv_importer, csv_importer_full\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn import tree, metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to find demarcator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_demarcator(dataset):\n",
    "    \"\"\"\n",
    "    Restituisce l'indice del primo elemento non fake\n",
    "    :param dataset: il dataset\n",
    "    :return: l'indice\n",
    "    \"\"\"\n",
    "    idx = 0\n",
    "    for elem in dataset:\n",
    "        if elem['fake'] == 1:\n",
    "            idx += 1\n",
    "        else:\n",
    "            break\n",
    "    return idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set train:test ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENT_TRAIN = 70"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading from file ./dataset/sources/user_fake_authentic_2class.csv...\n",
      "Loaded 65327 entries from source ./dataset/sources/user_fake_authentic_2class.csv\n"
     ]
    }
   ],
   "source": [
    "default_dataset = csv_importer_full(\"./dataset/sources/user_fake_authentic_2class.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into (balanced) training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now splitting dataset with ratio 70:30\n",
      "Loading complete.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Now splitting dataset with ratio {PERCENT_TRAIN}:{100 - PERCENT_TRAIN}\")\n",
    "\n",
    "# Find demarcator (in the original datasets all fake accounts are at the beginning)\n",
    "idx = find_demarcator(default_dataset)\n",
    "# Separate fakes from real accounts\n",
    "fake = default_dataset[:idx]\n",
    "correct = default_dataset[idx:]\n",
    "# Shuffle both datatets (otherwise, train and validation sets would always contain the same elements)\n",
    "random.shuffle(fake)\n",
    "random.shuffle(correct)\n",
    "# Create training set\n",
    "train = fake[:int(len(fake) * (PERCENT_TRAIN / 100))]\n",
    "train += correct[:int(len(correct) * (PERCENT_TRAIN / 100))]\n",
    "# Create validation set\n",
    "validation = fake[int(len(fake) * (PERCENT_TRAIN / 100)):]\n",
    "validation += correct[int(len(correct) * (PERCENT_TRAIN / 100)):]\n",
    "# Shuffle both datasets\n",
    "random.shuffle(train)\n",
    "random.shuffle(validation)\n",
    "\n",
    "print(\"Loading complete.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast to pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       nmedia     flw     flg   biol  pic  url      cl        cz     ni  \\\n",
      "0        95.0   733.0   420.0  157.0  1.0  0.0   238.0  0.000000  0.333   \n",
      "1         0.0    80.0    72.0    0.0  1.0  0.0     0.0  0.000000  0.000   \n",
      "2        26.0   106.0   288.0    0.0  1.0  0.0     0.0  1.000000  0.167   \n",
      "3         5.0   184.0  2600.0    0.0  1.0  0.0     0.0  1.000000  0.000   \n",
      "4       649.0   689.0  7400.0   88.0  1.0  1.0     0.0  1.000000  0.000   \n",
      "...       ...     ...     ...    ...  ...  ...     ...       ...    ...   \n",
      "45723    89.0   206.0  6000.0   95.0  1.0  0.0  1437.0  0.055556  0.000   \n",
      "45724     1.0    19.0   423.0    0.0  1.0  0.0     0.0  1.000000  0.000   \n",
      "45725     5.0    93.0  7100.0    4.0  1.0  0.0    12.0  0.600000  0.400   \n",
      "45726    45.0  2300.0   667.0  145.0  1.0  0.0    76.0  0.000000  0.778   \n",
      "45727   133.0   371.0   852.0  134.0  1.0  0.0    74.0  0.000000  0.333   \n",
      "\n",
      "             erl   erc     lt    ahc   pr   fo        cs      avgtime  fake  \n",
      "0      21.540001  1.03  0.944  2.056  0.0  0.0  0.097681   383.478790     0  \n",
      "1       0.000000  0.00  0.000  0.000  0.0  0.0  1.000000     0.000000     1  \n",
      "2       6.340000  0.58  0.056  0.000  0.0  0.0  1.000000   354.777924     1  \n",
      "3      13.370000  1.20  0.000  0.000  0.0  0.0  1.000000   503.932892     0  \n",
      "4       9.340000  0.62  0.000  0.000  0.0  0.0  1.000000   244.451019     1  \n",
      "...          ...   ...    ...    ...  ...  ...       ...          ...   ...  \n",
      "45723  17.389999  0.13  0.000  0.000  0.0  0.0  0.784314   133.904999     1  \n",
      "45724   0.000000  5.26  0.000  0.000  0.0  0.0  0.000000     0.000000     1  \n",
      "45725  28.820000  1.08  0.600  0.000  0.0  0.0  0.367082  1044.425171     1  \n",
      "45726  11.840000  0.78  0.000  0.333  0.0  0.0  0.044953   196.219910     0  \n",
      "45727   1.630000  0.12  0.000  0.000  0.0  0.0  0.099145   287.037109     1  \n",
      "\n",
      "[45728 rows x 18 columns]\n",
      "       nmedia     flw     flg  biol  pic  url     cl        cz     ni  \\\n",
      "0         7.0   103.0  7300.0   0.0  1.0  0.0    0.0  1.000000  0.000   \n",
      "1        76.0   714.0   341.0  40.0  1.0  0.0  665.0  0.000000  0.056   \n",
      "2         0.0   126.0  6500.0   0.0  0.0  0.0    0.0  0.000000  0.000   \n",
      "3       103.0  1800.0  7500.0   0.0  1.0  0.0   13.0  0.777778  0.000   \n",
      "4         2.0   720.0  3200.0   0.0  1.0  0.0    0.0  1.000000  0.500   \n",
      "...       ...     ...     ...   ...  ...  ...    ...       ...    ...   \n",
      "19593     4.0    62.0   223.0   8.0  1.0  1.0   12.0  0.250000  0.000   \n",
      "19594   242.0   722.0   788.0  33.0  1.0  0.0  418.0  0.000000  0.167   \n",
      "19595     3.0    91.0  7400.0   0.0  1.0  0.0    5.0  0.666667  0.000   \n",
      "19596     1.0    48.0   109.0   0.0  1.0  0.0   64.0  0.000000  0.000   \n",
      "19597    30.0   315.0   634.0   0.0  1.0  0.0    0.0  1.000000  0.000   \n",
      "\n",
      "             erl   erc    lt    ahc     pr     fo        cs      avgtime  fake  \n",
      "0      26.070000  0.97  0.00  0.000  0.000  0.000  1.000000   187.153732     1  \n",
      "1       4.540000  0.26  0.00  0.111  0.611  0.167  0.177766    62.478966     1  \n",
      "2       0.000000  0.00  0.00  0.000  0.000  0.000  1.000000     0.000000     1  \n",
      "3       1.870000  0.06  0.00  0.000  0.000  0.000  0.594771   420.081299     1  \n",
      "4       7.780000  0.69  0.00  0.000  0.000  0.000  1.000000  8372.947266     1  \n",
      "...          ...   ...   ...    ...    ...    ...       ...          ...   ...  \n",
      "19593  11.690000  0.40  0.25  0.000  0.000  0.000  0.166667    19.733402     0  \n",
      "19594   1.770000  0.22  0.00  0.056  0.389  0.278  0.064511   228.037903     1  \n",
      "19595  71.059998  6.59  0.00  0.000  0.000  0.000  0.333333   220.565735     1  \n",
      "19596  45.830002  6.25  0.00  0.000  0.000  0.000  0.000000     0.000000     0  \n",
      "19597   0.090000  0.00  0.00  0.000  0.000  0.000  1.000000     3.910247     1  \n",
      "\n",
      "[19598 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame.from_dict(train)\n",
    "validation_df = pd.DataFrame.from_dict(validation)\n",
    "print(train_df)\n",
    "print(validation_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting complete.\n"
     ]
    }
   ],
   "source": [
    "# Default tree\n",
    "X, y = train_df.iloc[:, :-2], train_df.iloc[:, -1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "print(\"Fitting complete.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86      9738\n",
      "           1       0.86      0.86      0.86      9860\n",
      "\n",
      "    accuracy                           0.86     19598\n",
      "   macro avg       0.86      0.86      0.86     19598\n",
      "weighted avg       0.86      0.86      0.86     19598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = validation_df.iloc[:, :-2], validation_df.iloc[:, -1]\n",
    "y_pred = clf.predict(X_val)\n",
    "print(metrics.classification_report(y_val,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First experiment with custom features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading from file ./dataset/sources/user_fake_authentic_2class.csv...\n",
      "Loaded 65327 entries from source ./dataset/sources/user_fake_authentic_2class.csv\n",
      "Loading complete.\n",
      "Fitting complete.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.49      0.49      9738\n",
      "           1       0.50      0.51      0.50      9860\n",
      "\n",
      "    accuracy                           0.50     19598\n",
      "   macro avg       0.50      0.50      0.50     19598\n",
      "weighted avg       0.50      0.50      0.50     19598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_dataset = csv_importer(\"./dataset/sources/user_fake_authentic_2class.csv\")\n",
    "\n",
    "custom_fake = custom_dataset[:idx]\n",
    "custom_correct = custom_dataset[idx:]\n",
    "\n",
    "random.shuffle(custom_fake)\n",
    "random.shuffle(custom_correct)\n",
    "\n",
    "custom_train = custom_fake[:int(len(custom_fake) * (PERCENT_TRAIN / 100))]\n",
    "custom_train += custom_correct[:int(len(custom_correct) * (PERCENT_TRAIN / 100))]\n",
    "\n",
    "custom_validation = custom_fake[int(len(custom_fake) * (PERCENT_TRAIN / 100)):]\n",
    "custom_validation += custom_correct[int(len(custom_correct) * (PERCENT_TRAIN / 100)):]\n",
    "\n",
    "random.shuffle(custom_train)\n",
    "random.shuffle(custom_validation)\n",
    "\n",
    "print(\"Loading complete.\")\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(train)\n",
    "validation_df = pd.DataFrame.from_dict(validation)\n",
    "#print(train_df)\n",
    "#print(validation_df)\n",
    "\n",
    "custom_train_df = pd.DataFrame.from_dict(custom_train)\n",
    "custom_validation_df = pd.DataFrame.from_dict(custom_validation)\n",
    "#print(custom_train_df)\n",
    "#print(custom_validation_df)\n",
    "\n",
    "# Custom tree\n",
    "cX, cy = custom_train_df.iloc[:,:-2], custom_train_df.iloc[:,-1]\n",
    "cclf = tree.DecisionTreeClassifier()\n",
    "cclf = cclf.fit(cX, cy)\n",
    "print(\"Fitting complete.\")\n",
    "\n",
    "cX_val, cy_val = custom_validation_df.iloc[:,:-2], validation_df.iloc[:, -1]\n",
    "cy_pred = cclf.predict(cX_val)\n",
    "\n",
    "print(metrics.classification_report(cy_val,cy_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate impact upon removing single-attributes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nmedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['nmedia', 'flw', 'flg', 'biol', 'pic', 'url', 'cl', 'cz', 'ni', 'erl',\n",
      "       'erc', 'lt', 'ahc', 'pr', 'fo', 'cs', 'avgtime', 'fake'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)\n",
    "#print(train_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove nmedia column from training and validation dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drop_nmedia = train_df.drop(['nmedia'], axis=1)\n",
    "validation_drop_nmedia = validation_df.drop(['nmedia'], axis=1)\n",
    "#print(train_drop_nmedia)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting complete.\n"
     ]
    }
   ],
   "source": [
    "X, y = train_df.iloc[:, :-2], train_df.iloc[:, -1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "print(\"Fitting complete.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86      9738\n",
      "           1       0.86      0.86      0.86      9860\n",
      "\n",
      "    accuracy                           0.86     19598\n",
      "   macro avg       0.86      0.86      0.86     19598\n",
      "weighted avg       0.86      0.86      0.86     19598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = validation_df.iloc[:, :-2], validation_df.iloc[:, -1]\n",
    "y_pred = clf.predict(X_val)\n",
    "print(metrics.classification_report(y_val,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
